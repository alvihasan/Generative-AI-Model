{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOUiRlWc81xvFp3Jnp04IBt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["LangChain is a framework designed to simplify the development of applications that use large language models (LLMs). It helps developers connect LLMs with external data sources, handle prompt engineering, manage memory across interactions, and create complex workflows. Here’s a beginner-level overview:\n","\n","### 1. **Purpose of LangChain**:\n","   - LangChain helps create **end-to-end LLM-powered applications**, integrating language models with other services and data. It focuses on **chaining** different components and **orchestrating workflows** involving language models.\n","\n","### 2. **Key Components**:\n","   - **LLMs and Prompts**: LangChain helps developers interact with LLMs like OpenAI's GPT models. It makes it easy to create, optimize, and use prompts effectively.\n","   - **Chains**: Chains are sequences of calls that connect LLMs to other tools, APIs, or workflows. For example, a chain might take user input, fetch relevant data, and then format a response using an LLM.\n","   - **Agents**: Agents are workflows where an LLM acts as a decision-maker, determining which actions to take in response to user queries. They use a **toolkit**, such as accessing databases, APIs, or other models.\n","   - **Memory**: LangChain provides memory modules, allowing applications to **remember context** between interactions, enabling more **dynamic and interactive conversations**.\n","\n","### 3. **Why Use LangChain?**:\n","   - **Simplifies Prompt Management**: LangChain helps manage and optimize the prompts sent to LLMs, ensuring consistent and effective communication.\n","   - **Tool Integration**: It allows seamless integration of LLMs with other external services or custom tools, like web search, databases, or even external APIs.\n","   - **Complex Workflow Orchestration**: It makes developing more complex workflows involving LLMs straightforward by offering different pre-built components like chains and agents.\n","   - **Handling Context**: By using memory, LangChain helps to maintain the conversation’s context, which is useful in building chatbots or assistants that require multiple exchanges.\n","\n","### 4. **Example Use Cases**:\n","   - **Chatbots**: You can use LangChain to build intelligent chatbots that maintain context and access relevant information when needed.\n","   - **Question Answering Systems**: LangChain can chain LLMs with tools that fetch information from the web or databases to answer user questions accurately.\n","   - **Content Generation**: You can create pipelines that generate content, perform edits, and verify facts in a single workflow."],"metadata":{"id":"u0326cD973sT"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TUY9p47MHcZQ","executionInfo":{"status":"ok","timestamp":1727776528458,"user_tz":-360,"elapsed":19023,"user":{"displayName":"Md. Ashraful Islam","userId":"10804497249634088005"}},"outputId":"a3fb9cd1-7bb8-44d6-b9a3-847a4c8e958a"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.2/292.2 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["# Installation\n","!pip install -qU langchain\n","!pip install -qU langchain-groq"]},{"cell_type":"code","source":["# Import\n","import getpass\n","import os\n","from langchain_groq import ChatGroq\n","from langchain_core.messages import HumanMessage, SystemMessage\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.prompts import ChatPromptTemplate"],"metadata":{"id":"CC0SYrxsH0hJ","executionInfo":{"status":"ok","timestamp":1727776530253,"user_tz":-360,"elapsed":1802,"user":{"displayName":"Md. Ashraful Islam","userId":"10804497249634088005"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# API Key setup\n","\n","# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n","# os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()\n","\n","os.environ[\"GROQ_API_KEY\"] = getpass.getpass()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GQrAcz3VHuEM","executionInfo":{"status":"ok","timestamp":1727776540304,"user_tz":-360,"elapsed":10055,"user":{"displayName":"Md. Ashraful Islam","userId":"10804497249634088005"}},"outputId":"9d6bea4f-ae08-402a-83f2-601730ab68cc"},"execution_count":3,"outputs":[{"name":"stdout","output_type":"stream","text":["··········\n"]}]},{"cell_type":"code","source":["model = ChatGroq(model=\"llama3-8b-8192\")"],"metadata":{"id":"qzOnWtHuIA4z","executionInfo":{"status":"ok","timestamp":1727776558851,"user_tz":-360,"elapsed":528,"user":{"displayName":"Md. Ashraful Islam","userId":"10804497249634088005"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Let's first use the model directly.\n","ChatModels are instances of LangChain \"Runnables\",\n","which means they expose a standard interface for interacting with them.\n","To just simply call the model, we can pass in a list of messages to the .invoke method.\n","\"\"\"\n","\n","messages = [\n","    SystemMessage(content=\"Translate the following from English into Bangla\"),\n","    HumanMessage(content=\"how are you?\"),\n","]\n","\n","model.invoke(messages)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0WG_-1xNICKl","executionInfo":{"status":"ok","timestamp":1727776670363,"user_tz":-360,"elapsed":543,"user":{"displayName":"Md. Ashraful Islam","userId":"10804497249634088005"}},"outputId":"0c138bbc-2114-4e02-e4d8-bb6abe1209f3"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='আপনি কেমন আছেন?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 27, 'total_tokens': 47, 'completion_time': 0.01686812, 'prompt_time': 0.005255934, 'queue_time': 0.008998856, 'total_time': 0.022124054}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_6a6771ae9c', 'finish_reason': 'stop', 'logprobs': None}, id='run-c3cb8cbc-7604-4ebd-9043-7c25cb46906e-0', usage_metadata={'input_tokens': 27, 'output_tokens': 20, 'total_tokens': 47})"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["# OutputParsers\n","\"\"\"\n","Notice that the response from the model is an AIMessage.\n","This contains a string response along with other metadata about the response.\n","Oftentimes we may just want to work with the string response.\n","We can parse out just this response by using a simple output parser.\n","\"\"\"\n","\n","parser = StrOutputParser()"],"metadata":{"id":"_f0RS9rZIhhp","executionInfo":{"status":"ok","timestamp":1727776723296,"user_tz":-360,"elapsed":537,"user":{"displayName":"Md. Ashraful Islam","userId":"10804497249634088005"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Let us use both\n","\n","result = model.invoke(messages)\n","\n","parser.invoke(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"Cj8EEtvNIqEP","executionInfo":{"status":"ok","timestamp":1727776726914,"user_tz":-360,"elapsed":539,"user":{"displayName":"Md. Ashraful Islam","userId":"10804497249634088005"}},"outputId":"d852693d-64bc-49e7-d436-5ca038d4d80a"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'আপনি কেমন আছেন? (Aponi kemon achhen?)'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["# Chaining\n","\n","\"\"\"\n","More commonly, we can \"chain\" the model with this output parser.\n","This means this output parser will get called every time in this chain.\n","This chain takes on the input type of the language model (string or list of message)\n","and returns the output type of the output parser (string).\n","\n","We can easily create the chain using the | operator.\n","The | operator is used in LangChain to combine two elements together.\n","\"\"\"\n","\n","chain = model | parser\n","\n","chain.invoke(messages)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"8FVnNsZSIvV4","executionInfo":{"status":"ok","timestamp":1727776791061,"user_tz":-360,"elapsed":494,"user":{"displayName":"Md. Ashraful Islam","userId":"10804497249634088005"}},"outputId":"8a5bd1aa-86e4-4ba4-b5aa-3aaba3d86811"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'আপনি কেমন আছেন? (Aponi kemon achhen?)'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["# Prompt Templates\n","\"\"\"\n","PromptTemplates are a concept in LangChain designed to assist with this transformation.\n","They take in raw user input and return data (a prompt) that is ready to pass into a language model.\n","\n","Let's create a PromptTemplate here. It will take in two user variables:\n","\n","language: The language to translate text into\n","text: The text to translate\n","\"\"\"\n","\n","prompt_template = ChatPromptTemplate.from_messages(\n","    [\n","        (\"system\", \"Translate the following into {language}:\"),\n","        (\"user\", \"{text}\")\n","    ]\n",")"],"metadata":{"id":"_GPUP5dBJiXD","executionInfo":{"status":"ok","timestamp":1727776912084,"user_tz":-360,"elapsed":847,"user":{"displayName":"Md. Ashraful Islam","userId":"10804497249634088005"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["result = prompt_template.invoke({\"language\": \"Bangla\", \"text\": \"how are you?\"})\n","\n","result"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QR9kNdlFJywD","executionInfo":{"status":"ok","timestamp":1727776914220,"user_tz":-360,"elapsed":8,"user":{"displayName":"Md. Ashraful Islam","userId":"10804497249634088005"}},"outputId":"e1dd2457-257e-4234-f311-0792a9492c38"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ChatPromptValue(messages=[SystemMessage(content='Translate the following into Bangla:', additional_kwargs={}, response_metadata={}), HumanMessage(content='how are you?', additional_kwargs={}, response_metadata={})])"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["\"\"\"\n","We can see that it returns a ChatPromptValue that consists of two messages.\n","If we want to access the messages directly we do:\n","\"\"\"\n","\n","result.to_messages()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tGsMbKMwJ8as","executionInfo":{"status":"ok","timestamp":1727776950521,"user_tz":-360,"elapsed":520,"user":{"displayName":"Md. Ashraful Islam","userId":"10804497249634088005"}},"outputId":"39d0dd20-d301-4bd5-ace3-d6bfeeae36ec"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[SystemMessage(content='Translate the following into Bangla:', additional_kwargs={}, response_metadata={}),\n"," HumanMessage(content='how are you?', additional_kwargs={}, response_metadata={})]"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["# Combining All\n","\"\"\"\n","We can now combine this with the model and the output parser from above using the pipe (|) operator:\n","\"\"\"\n","chain = prompt_template | model | parser\n","\n","chain.invoke({\"language\": \"Bangla\", \"text\": \"how are you\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"-Qbc-5ItKEmq","executionInfo":{"status":"ok","timestamp":1727776982469,"user_tz":-360,"elapsed":538,"user":{"displayName":"Md. Ashraful Islam","userId":"10804497249634088005"}},"outputId":"d41b2fa0-7f99-4d64-b3ea-8082ad6c8d26"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'আপনার কেমন আছেন?'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["# Serving with LangServe\n","\n","# !pip install \"langserve[all]\""],"metadata":{"id":"BQcdCaQZKPkw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Server code\n","\n","#!/usr/bin/env python\n","import getpass\n","import os\n","from fastapi import FastAPI\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_groq import ChatGroq\n","from langserve import add_routes\n","\n","os.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n","\n","# 1. Create prompt template\n","system_template = \"Translate the following into {language}:\"\n","prompt_template = ChatPromptTemplate.from_messages([\n","    ('system', system_template),\n","    ('user', '{text}')\n","])\n","\n","# 2. Create model\n","model = ChatGroq(model=\"llama3-8b-8192\")\n","\n","# 3. Create parser\n","parser = StrOutputParser()\n","\n","# 4. Create chain\n","chain = prompt_template | model | parser\n","\n","\n","# 4. App definition\n","app = FastAPI(\n","  title=\"LangChain Server\",\n","  version=\"1.0\",\n","  description=\"A simple API server using LangChain's Runnable interfaces\",\n",")\n","\n","# 5. Adding chain route\n","add_routes(\n","    app,\n","    chain,\n","    path=\"/chain\",\n",")\n","\n","if __name__ == \"__main__\":\n","    import uvicorn\n","\n","    uvicorn.run(app, host=\"localhost\", port=8000)"],"metadata":{"id":"c-hy_X-lKRuu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Commands\n","# python serve.py"],"metadata":{"id":"HI8N-X1oKYj0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Client code\n","from langserve import RemoteRunnable\n","\n","remote_chain = RemoteRunnable(\"http://localhost:8000/chain/\")\n","remote_chain.invoke({\"language\": \"Bangla\", \"text\": \"how are you\"})"],"metadata":{"id":"fuk6dMkdKcxK"},"execution_count":null,"outputs":[]}]}